\section{Related work}
\label{sec:related_work}

Shapley values have been applied to explain behaviour \citep{Rizzo2019, Carbone2020, He2020, Wang2020, Liessner2021, Lover2021, Remman2021, Theumer2022} and prediction \citep{Zhang2020, Schreiber2021, Zhang2022} in reinforcement learning. \citet{Beechey2023, Beechey2025} studied the theoretical validity of Shapley values in reinforcement learning. Their analysis revealed outcomes as a missing explanatory element, unifying behaviour, outcomes, and prediction under a single theoretical framework, SVERL. We build on this earlier foundation to develop scalable approximation techniques, which make it possible to use SVERL in practical settings. 

In supervised learning, Shapley values were initially approximated through sampling \citep{Strumbelj2009, Strumbelj2010, Strumbelj2014, Lundberg2017}. More recent approaches improve efficiency by amortising computation across inputs with parametric models \citep{Frye2020, Jethani2021} that incorporate noisy Shapley targets during training \citep{Covert2024}. Our work incorporates the use of these amortised methods into reinforcement learning, addressing practical constraints such as off-policy data and continual learning. In addition, we introduce a single-sample noisy target that can eliminate the need for characteristic models, substantially improving training efficiency.