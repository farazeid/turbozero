\section{Discussion}
\label{sec:discussion}

We introduced FastSVERL, a scalable parametric framework for Shapley-based explanations of reinforcement learning. FastSVERL estimates Shapley values in a single forward pass, without relying on costly Monte Carlo samples. We addressed two practical challenges: (i) off-policy learning, to enable explanations with minimal environment interaction, and (ii) continual learning, to allow explanations of non-stationary policies in real time. We showed that replacing characteristic models with single-sampled approximations can substantially improve efficiency and explanation quality; this approach also directly extends to supervised learning. These contributions position FastSVERL as a scalable solution for interpretability in practical reinforcement learning problems.

The proposed methods are broadly applicable to episodic and continuous tasks, as well as partially observable settings. Although this work focuses on discrete action spaces, the underlying theoretical framework supports continuous actions, and FastSVERL's parametric approach readily adapts. Applying the framework to continuous state spaces, however, highlights a key challenge: approximating the steady-state distribution. In high-dimensional settings, experience buffers provide only a sparse approximation of the true distribution. One promising direction is to learn a parametric model of this distribution \citep{Frye2020}, which could generalise across sparsely sampled regions.

Our work focused on the significant computational challenges of approximating Shapley values. Formal user studies and practical deployment in real-world systems are essential to evaluate how these explanations aid human understanding and decision-making. While our qualitative results suggest the framework can produce plausibly interpretable insights, user studies can rigorously evaluate robustness, identify use cases, and reveal the conditions under which the approximations become unreliable. This next step is critical for establishing best practices and strengthening FastSVERL's capacity for scalable, real-time interpretability in complex reinforcement learning environments.