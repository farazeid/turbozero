\section{Domains}  
\label{app:domains}

This section provides descriptions of the reinforcement learning domains used in the experiments.

\textbf{Gridworld} \citep{Beechey2025}, is a deterministic $2 \times 4$ environment where the agent's state is defined by its $(x, y)$ grid coordinates. The state space is:  
\begin{equation}  
\S = \{(1,1), (1,3), (1,4), (2,1), (2,2), (2,3), (2,4)\}.
\end{equation}
Each episode begins in a start state sampled uniformly from $(1,1)$ and $(2,1)$. The agent can take actions \texttt{North}, \texttt{East}, \texttt{South}, and \texttt{West}; actions that would move the agent outside the grid or into the missing state $(1,2)$ are treated as invalid, incurring a reward but leaving the agent's position unchanged.

The terminal states are $(1,4)$ and $(2,4)$. The agent receives a reward of $-1$ per decision stage and an additional $+10$ upon reaching a terminal state, making this a shortest-path problem. The optimal policy moves \texttt{East} in $(1,1)$ and \texttt{North} in all other states to minimise the number of steps to termination.

\textbf{Mastermind} \citep{Beechey2025} is a reinforcement learning adaptation of the classic code-breaking game. At the start of each episode, the environment randomly samples a hidden code consisting of a sequence of letters. The agent must identify the code within a fixed number of guesses. After each guess, the environment returns two types of feedback:
\begin{itemize}
    \item \textbf{Position clue:} the number of letters that are both correct and in the correct position.
    \item \textbf{Misplaced clue:} the number of correct letters that are in the wrong position.
\end{itemize}
These values are computed sequentially: letters that are correct and in the correct position contribute to the position clue and are excluded when computing the misplaced clue, which counts only the remaining correct letters in the wrong position.

Each state encodes the agentâ€™s current game board: a sequence of previous guesses and the corresponding feedback. States are feature-based, with each guess represented by a fixed number of features: one per letter in the guess, plus two features for the position and misplaced clues. Unused guesses are represented using a dedicated \texttt{empty} value.

The environment can be configured by varying the code length, the number of guesses, and the size of the letter set (i.e. the alphabet). We consider five configurations:
\begin{itemize}
    \item \texttt{Mastermind-222}: Codes of length 2 drawn from the alphabet $\{\mathrm{A}, \mathrm{B}\}$, yielding 4 possible codes. The agent is allowed up to 2 guesses. The state space contains 53 unique states, each represented by 8 features. Each code corresponds to a unique action, giving 4 available actions.
    \item \texttt{Mastermind-333}: Codes of length 3 drawn from the alphabet $\{\mathrm{A}, \mathrm{B}, \mathrm{C}\}$, yielding 27 possible codes. The agent is allowed up to 3 guesses. The state space contains over 100{,}000 states, each with 15 features. There are 27 available actions.
    \item \texttt{Mastermind-443}: Codes of length 4 drawn from the alphabet $\{\mathrm{A}, \mathrm{B}, \mathrm{C}\}$, yielding 81 possible codes. The agent is allowed up to 4 guesses. The state space contains over $4.3 \times 10^7$ states, each with 24 features. There are $81$ available actions.
    \item \texttt{Mastermind-453}: Codes of length 4 drawn from the alphabet $\{\mathrm{A}, \mathrm{B}, \mathrm{C}\}$, yielding 81 possible codes. The agent is allowed up to 5 guesses. The state space contains over $3.5 \times 10^9$ states, each with 30 features. There are $81$ available actions.
    \item \texttt{Mastermind-463}: Codes of length 4 drawn from the alphabet $\{\mathrm{A}, \mathrm{B}, \mathrm{C}\}$, yielding 81 possible codes. The agent is allowed up to 6 guesses. The state space contains over $2.8 \times 10^{11}$ states, each with 36 features. There are $81$ available actions.
\end{itemize}

The reward function assigns $-1$ per guess and provides an additional reward equal to the maximum number of guesses if the agent correctly identifies the hidden code. Episodes terminate when the correct code is guessed or the guess limit is reached.