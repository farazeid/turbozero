\section{Approximating Shapley values without characteristic models}
\label{sec:sampling}

We introduced FastSVERL, a model-based framework to approximate Shapley values in reinforcement learning, providing a solid foundation upon which the community can build. Here, we present one such promising extension. FastSVERL uses parametric approximations of characteristic functions to train Shapley models. Training these characteristic models is a major computational bottleneck, with the \texttt{Hypercube} experiment in \cref{fig:hypercub_policy_scaling} suggesting they require as much computation as training the Shapley models themselves. This motivates the search for alternative approximation methods. Monte Carlo sampling might be an appealing alternative because it removes the need for parametric models entirely. However, this merely shifts the computational burden from model training to the high cost of repeated sampling. What if we could preserve the model-free benefits of sampling without incurring its computational cost? We propose integrating single-sample approximations of characteristic values (with low computational cost) directly into the Shapley model loss, amortising characteristic estimation within Shapley training itself. We illustrate this idea using behaviour explanations.

We replace the behaviour characteristic $\tilde\pi_s^a(\C)$ in the Shapley model loss (\cref{eq:fastsverl}) with a single-sample approximation. At each loss evaluation, we sample a state $s' \sim p^\pi(\cdot \mid s^\C)$ and use the action probability $\pi(s', a)$ in place of querying a characteristic model:
\begin{equation} \label{eq:policy-mc}
    \mathcal{L}(\theta) = \underset{p^\pi(s)}{\mathbb{E}}\,\underset{\text{Unif}(a)}{\mathbb{E}}\,\underset{p(\mathcal{C})}{\mathbb{E}}\,\underset{s' \sim p^\pi(\,\cdot\,|\,s^\C)}{\mathbb{E}} \Big| \pi(s', a) - \pi_{s,a}(\emptyset) - \sum_{i\in\C} \hat{\phi}^i(s, a; \theta) \Big|^2. 
\end{equation}
In \cref{app:sampling}, we prove that optimising this new loss recovers the exact and unbiased Shapley values, with the same per-update cost as the original loss in \cref{eq:fastsverl}. This approach presents a favourable trade-off: we accept higher variance in individual gradients to eliminate the cost of pre-training a characteristic model and prevent its approximation errors from propagating into the Shapley model.

We illustrate the proposed approach by training a behaviour Shapley model in \texttt{Mastermind-222} with three characteristic approximations: (1) single-samples, (2) model-based, and (3) exact values. \cref{fig:sampling} shows the Shapley model's loss over cumulative training updates, reflecting characteristic and Shapley model training. The model-based curve starts later because its initial updates are dedicated to learning the characteristic model (indicated by the dashed line), and it converges to a higher loss, reflecting approximation errors in the characteristic. In contrast, sampling converges before the model-based setup begins its Shapley updates, matching the performance of using \emph{exact characteristic values}. Interestingly, the model trained with exact values converges slightly slower. As observed in prior work \citep{Covert2024}, the noise from the single-sample approximations may act as a stochastic regulariser, encouraging more efficient learning by preventing the model from overfitting to a single batch. These results show how replacing characteristic models with sampling can improve Shapley model efficiency and accuracy: halving total training time and eliminating error propagation.