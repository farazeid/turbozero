\section{Introduction}
\label{sec:introduction}

Reinforcement learning has achieved remarkable success in complex decision-making environments \citep{Bellemare2020, Seo2024}, but the lack of transparency in an agent's decisions limits its deployment in practice, especially in safety-critical settings. While various interpretability methods have been proposed \citep{Greydanus2018, Puri2019}, they often lack theoretical guarantees. A principled and promising approach \citep{Beechey2023, Beechey2025} is the use of Shapley values \citep{Shapley1953} to attribute the influence of features---numerical values that describe what an agent observes in its environment---on the agent's \emph{behaviour}, \emph{outcomes}, and \emph{predictions}. Grounding these attributions in Shapley values provides a principled method for fair, transparent, and consistent credit assignment. One significant difficulty is that the computational cost of the Shapley approach scales exponentially with the number of features, making its application impractical for most real-world problems. There is therefore a strong need to develop scalable approximation methods that enable practical Shapley-based interpretability in reinforcement learning.

In supervised learning, scalable approximations of Shapley values are achieved through two primary methods: sampling over feature subsets \citep{Strumbelj2009, Strumbelj2010, Strumbelj2014, Lundberg2017} and parametric models that amortise computation across inputs \citep{Frye2020, Jethani2021, Covert2024}. These methods are designed primarily for single-step predictions, whereas reinforcement learning introduces temporal dependencies across sequences of decisions. For example, explaining an agent's \emph{outcomes} requires attributing influence over its expected return, which accumulates across possibly an infinite number of decisions. Furthermore, (1) as an agent's behaviour evolves, the explanations must evolve with it, and (2) when environment interaction is limited, explanations must be approximated from off-policy data---gathered by agents acting differently from the agent being explained. Addressing these challenges requires approximation methods tailored to the multi-stage, interactive, and evolving nature of reinforcement learning. We develop such an approach here, which we call \emph{FastSVERL}.%
\footnote{FastSVERL code is available at: \url{https://github.com/djeb20/fastsverl}.}

We introduce a scalable parametric learning method for approximating Shapley values in reinforcement learning. The proposed approach is designed to handle the temporal dependencies inherent to reinforcement learning, amortising Shapley value estimation across multi-step trajectories to explain agent behaviour, outcomes, and predictions efficiently. We address the practical constraints introduced in real-world reinforcement learning by considering how to explain policies from off-policy data whilst adapting them to evolving agent behaviours. The comprehensive, model-based framework we introduce provides a solid foundation that the research community can build upon. We present one such promising extension in \cref{sec:sampling}, which preliminary findings suggest halves computational costs, improves estimation accuracy, and extends naturally to supervised learning.  

These contributions position FastSVERL as a principled, rigorous, and scalable solution for interpretability in the practice of reinforcement learning.