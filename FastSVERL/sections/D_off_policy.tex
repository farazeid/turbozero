\section{Off-policy learning for explanation models}
\label{app:off_policy}

This section presents the full derivation of the off-policy importance sampling approach introduced in \cref{sec:extending_fastsverl}, along with a complete set of empirical illustrations.

\subsection{Theory and derivation}

FastSVERL trains characteristic and Shapley models using samples from the steady-state distribution $p^\pi(s)$ of the policy being explained. But what if the agent cannot interact with the environment to collect this data, for example, when further interaction is costly? In such settings, we propose sampling from a replay buffer $\mathcal{B}$ containing transitions gathered during training. 

Because the buffer $\mathcal{B}$ aggregates data from a sequence of past policies $\{\pi_t\}_{t=0}^T$, its marginal state distribution $p^{\mathcal{B}}(s)$ differs from the target distribution $p^\pi(s)$. To correct the distributional mismatch, we apply importance sampling \citep{Precup2000}, reweighting each sample drawn from $\mathcal{B}$. For any loss of the form
\begin{equation}
    \mathcal{L}(\beta) = \underset{s \sim p^\pi}{\mathbb{E}} \left[ \ell(s; \beta) \right],
\end{equation}
we rewrite the expectation under $p^\pi(s)$ using samples from $p^{\mathcal{B}}(s)$:
\begin{equation}
    \mathcal{L}(\beta) = \underset{s \sim p^\mathcal{B}}{\mathbb{E}} \left[ \frac{p^\pi(s)}{p^\mathcal{B}(s)} \cdot \ell(s;\beta) \right].
\end{equation}
This yields an unbiased estimate of the original loss. However, neither $p^\pi(s)$ nor $p^\mathcal{B}(s)$ is known explicitly, so we approximate the density ratio using action probabilities. Since each transition $(s_t, a_t) \in \mathcal{B}$ was generated by a known behaviour policy $\pi_t$, we propose estimating the importance weight as:
\begin{equation}\label{eq:app_offpolicy}
    \mathcal{L}(\beta) \approx \underset{(s_t, a_t) \sim \mathcal{B}}{\mathbb{E}} \left[ \frac{\pi(s_t, a_t)}{\pi_t(s_t, a_t)} \cdot \ell(s_t; \beta) \right].
\end{equation}
The reweighting in \cref{eq:app_offpolicy} estimates the loss by weighting each sample in proportion to its relevance under $\pi$.

Two practical factors may affect the stability and accuracy of the importance-weighted loss in \cref{eq:app_offpolicy}. The first is the similarity between the target policy $\pi$ and the past policies $\{\pi_t\}$ that generated the buffer. The more these policies differ, the higher the variance of the importance weights, increasing the variance of the loss estimate. The second is the buffer's coverage of the state distribution $p^\pi(s)$. If states commonly visited by $\pi$ are underrepresented, their corresponding loss terms may be poorly estimated. In practice, transitions from later-stage policies are more likely to resemble the final policy $\pi$, so additional interaction near the end of training may improve both policy similarity and state coverage in the buffer.

When importance weights exhibit high variance, techniques such as weight clipping \citep{Schulman2017} or adaptive weighting \citep{Sutton2018} can help stabilise training. We illustrate these strategies empirically in the next section.

\subsection{Empirical illustrations}

We present three empirical illustrations of off-policy training in FastSVERL. First, we extend the experiment from \cref{fig:offpolicy_char} by training prediction characteristic models off-policy in \texttt{Mastermind-222}. We then investigate the impact of two variance-control strategies: adaptive weighting \citep{Sutton2018} and weight clipping \citep{Schulman2017}. All figures report approximation accuracy over training updates, measured by mean squared error (MSE) between predicted and exact values, averaged across states and features. Shaded regions indicate the standard error over 20 runs. Variability across runs is limited to randomness in the training pipeline, with all experimental conditions fixed and the entire process fully seeded for controlled reproducibility. Standard errors are corrected to account for this variability \citep{Masson2003}. 

\paragraph{Learning characteristics off-policy in \texttt{Mastermind-222}.}

\cref{fig:app_offpolicy_char} extends the illustration of off-policy training from \cref{fig:offpolicy_char} to prediction. Consistent with the findings in \cref{sec:extending_fastsverl}: (1) importance sampling reduces approximation error relative to using the training buffer without reweighting, suggesting that correcting for distributional mismatch improves performance; and (2) approximation error remains higher than under on-policy training, indicating that even with reweighting, off-policy data is an imperfect substitute.

\begin{figure*}[t]
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \includegraphics[width=\linewidth]{plots/app_mastermind_222_offpolicy_policy.pdf}
        \caption{Behaviour characteristic}
    \end{subfigure}
    \begin{subfigure}[t]{0.32\textwidth}
        \includegraphics[width=\linewidth]{plots/app_mastermind_222_offpolicy_value.pdf}
        \caption{Prediction characteristic}
    \end{subfigure}
    \caption{Approximation accuracy of off-policy training in \texttt{Mastermind-222} with models trained using either on-policy data, or off-policy data with two configurations: (1) without importance sampling (IS) and (2) with IS.
}
\label{fig:app_offpolicy_char}
\end{figure*}

\paragraph{Normalising importance weights.}

\begin{figure}
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \includegraphics[width=\linewidth]{plots/app_gwb_offpolicy_weighting_policy.pdf}
        \caption{Behaviour characteristic}
    \end{subfigure}
    \begin{subfigure}[t]{0.32\textwidth}
        \includegraphics[width=\linewidth]{plots/app_gwb_offpolicy_weighting_value.pdf}
        \caption{Prediction characteristic}
    \end{subfigure}
    \caption{Approximation accuracy of off-policy training in \texttt{Gridworld} with models trained using either on-policy data, or off-policy data with three configurations: (1) without importance sampling (IS), (2) with unnormalised IS, and (3) with normalised IS.
}
\label{fig:app_offpolicy_norm}
\end{figure}

A common strategy to reduce variance in importance sampling is to normalise weights within each batch by dividing them by their sum \citep{Sutton2018}. We illustrate this strategy by training behaviour and prediction characteristic models in \texttt{Gridworld} under three off-policy conditions: (1) without importance sampling, (2) with unnormalised importance sampling, and (3) with normalised importance sampling. An on-policy configuration is included as a baseline. The results, shown in \cref{fig:app_offpolicy_norm}, suggest that normalising the importance weights is crucial: using unnormalised weights leads to worse approximation accuracy than no reweighting at all, while normalised weights yield the best results. We therefore apply weight normalisation in all experiments using off-policy sampling.

\paragraph{Clipping importance weights.}

\begin{figure*}[t]
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \includegraphics[width=\linewidth]{plots/app_gwb_offpolicy_clipping_policy.pdf}
        \caption{Behaviour characteristic}
    \end{subfigure}
    \begin{subfigure}[t]{0.32\textwidth}
        \includegraphics[width=\linewidth]{plots/app_gwb_offpolicy_clipping_value.pdf}
        \caption{Prediction characteristic}
    \end{subfigure}
    \caption{Approximation accuracy of off-policy training in \texttt{Gridworld} with models trained using either on-policy data, or off-policy data with three configurations: (1) without importance sampling (IS), (2) with unclipped IS, and (3) with IS clipped at thresholds of 0.99, 0.995, and 0.998.
}
\label{fig:app_offpolicy_clip}
\end{figure*}

Another common strategy to reduce variance in importance sampling is to clip the weights to a fixed range \citep{Schulman2017}. We illustrate this strategy by training behaviour and prediction characteristic models in \texttt{Gridworld} under several off-policy conditions: (1) without importance sampling, (2) with unclipped importance sampling, and (3) with importance weights clipped symmetrically around $1$ to the range $[1 - c, 1 + c]$. We consider thresholds of $c = 0.99$, $0.995$, and $0.998$, selected to yield a spread of performance curves. An on-policy configuration is included as a baseline. The results, shown in \cref{fig:app_offpolicy_clip}, indicate that reducing variance through clipping may impair performance: unclipped importance sampling achieves the highest accuracy, while tighter clipping progressively harms approximation. The extreme case of $c = 0$, equivalent to no importance sampling, performs the worst. These results suggest a bias-variance trade-off in importance sampling, where retaining the full range of weights avoids the bias introduced by clipping and may be preferred to mitigating variance in this setting.

% Possibly a better alternative to the importance-sampling correction: train a discriminator to label $s_t$ from $B$ as negative, and $s_t$ from $\pi$ as positive. Then the importance sampling weight is roughly D(s) / (1 - D(s)). That can be plugged into the loss.